{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression, overfitting, and cross-validation\n",
    "\n",
    "Today we'll start our exploration of regression, focusing first on linear regression and ordinary least squares.  As we'll talk about in the lecture portion, we seek to estimate the relationship between a predictor or set of predictors and a predictand or response variable.  In doing so we will need to make judgements about the nature of this relationship and estimate the parameters or coefficients of a model that links the variables to one another.\n",
    "\n",
    "This notebook will walk us through some of the very basics of fitting (or training or calibrating) a linear regression model and doing some simple evaluation of the quality of our model.  We'll then introduce a few complications in the case where we have multiple potential predictors.  Finally, we'll introduce the concept of cross-validation. \n",
    "\n",
    "Let's get Numpy and Matplotlib first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# you can omit the line below if you'd like, but I really don't like the default fonts in Python, so I switch to Helvetica\n",
    "plt.rcParams['font.family'] = 'Helvetica'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to make use of the `scikit-learn` library.  Scikit-learn is broadly for machine learning, with regression being just one approach to machine learning.  Scikit-learn is not the only way to do regression in Python, of course.  Later will look at other options.  \n",
    "\n",
    "The documentation for scikit-learn can be found here: https://scikit-learn.org/stable/\n",
    "\n",
    "We'll specifically be using Linear Regression with in the `linear_model` module.\n",
    "\n",
    "Documentation for linear models is here: https://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "First, let's import what we need from scikit-learn.  We'll bring in `LinearRegression` from the `linear_model` module and then we'll get some metrics from the [`sklearn.metrics` module](sklearn.metrics).  Metrics are available for not only linear regression, but many other methods used in scikit-learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # the LinearRegression method from the linear_models module\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score # three different metrics we'll use to evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create two series of data, X and Y.  X will be the independent predictor variable, and Y will be the outcome or response or predictand variable that is dependent on X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's generate some synthetic data for the first simple example\n",
    "np.random.seed(1999) # once again, let Prince guide us to all getting the same answer in lecture - you can change this for your purposes whenever you want, however\n",
    "n_samples = 128 \n",
    "\n",
    "# create the independent variable as a random draw from the uniform distribution [0,1]\n",
    "X = 2 * np.random.rand(n_samples, 1)\n",
    "\n",
    "# create a little bit of random normal noise\n",
    "noise = np.random.randn(n_samples, 1) * 0.5 \n",
    "\n",
    "# set the beta parameters slope and intercept \n",
    "intercept = 4\n",
    "slope = 3\n",
    "\n",
    "# create the dependent variable \n",
    "y = intercept + (slope * X) + noise  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use `LinearRegression` to fit a model relating Y to X.  In scikit-learn, this process has several steps:\n",
    "\n",
    "1. Create the model - this doesn't do anything with the data yet, but rather creates a LinearRegression object\n",
    "2. We then call `model.fit`, which estimates the model coefficients between the predictors (X) and the predictand (y). You can see how `LinearRegression` works [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression).\n",
    "3. We will them predict the values of y from X.  This uses the model fit from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now, use scikit-learn's linear regression - first we create the model and assign it to 'model' ... \n",
    "model = LinearRegression()\n",
    "\n",
    "# then we use that model to fit the relationship between x and y\n",
    "model.fit(X, y)\n",
    "\n",
    "# then, we use this fit to predict y from x\n",
    "y_pred = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we can evaluate some of the metrics for our model.  We'll specifically look at $R^2$, the Mean Squared Error (MSE), and the Root Mean Squared Error (RMSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scikit-learn again to calculate the regression statistics or diagnostics\n",
    "r_squared = r2_score(y, y_pred)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = root_mean_squared_error(y, y_pred)\n",
    "\n",
    "# Display those statistics\n",
    "print(f\"R²: {r_squared:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'll discuss (briefly) in class, these metrics indicate a model that accounts for most of the variability in the predictand and has relatively small errors. \n",
    "\n",
    "Now, let's create a figure to evaluate the fit of our regression model, as well as to look at some of the assumptions of Ordinary Least Squares we looked at in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for our diagnostic plots ... \n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# first subplot is just the variables and the fit from the model\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X, y, color='blue', label='Observed Data')\n",
    "plt.plot(X, y_pred, color='red', label='Predicted Data')\n",
    "plt.title('X vs y with Regression Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# second plot is the predictor vs. the residuals (e.g. is the relationship between the predictor and errors in our model?)\n",
    "residuals = y - y_pred\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X, residuals, color='lightcoral')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('X vs Residuals')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# third plot is the lagged correlation of the residuals with themselves - is there a relationship between the errors? \n",
    "residuals_shifted = np.roll(residuals, 1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(residuals[1:], residuals_shifted[1:], color='dodgerblue')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('Lag-1 Residual Autocorrelation')\n",
    "plt.ylabel('Residual(t-1)')\n",
    "plt.xlabel('Residual(t)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show that our model fits the data well, that there is no detectable relationship between the predictors and the residuals of the model fit, and that the residuals do not have a trend or any significant autocorrelation.  This looks like a good model with a high $R^2$ and relatively small errors. \n",
    "\n",
    "As you can see, this is a relatively simple system.  We can look at the coefficient (in this case, just the slope) by calling the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_) # the coefficient(s) of the model\n",
    "print(model.intercept_) # the intercept of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these, you see they are quite close to the slope (3) and intercept (4) we specified when we created these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple predictors and the limitations of $R^2$\n",
    "\n",
    "Let's now consider a system with multiple predictors.  We are accustomed to thinking of $R^2$ as a regression metrics that tells us, essentially, 'how much of the variability in the response variable can be explained by the predictors?'.   But $R^2$ has some important limitations.  One of these is that as you add additional predictors to a multiple linear regression equation, $R^2$ will continue to increase (even if just marginally) with each predictor, even if the predictor has no relationship to the predictand!\n",
    "\n",
    "To guard against this, we introduce an adjusted $R^2$ statistic, which penalizes the $R^2$ calculation for each additional predictor.  \n",
    "\n",
    "$\n",
    "R^2_{\\text{adj}} = 1 - \\left( \\frac{1 - R^2}{n - 1} \\right) \\times (n - p - 1)\n",
    "$\n",
    "\n",
    "where $n$ is the number of observations in our datasets and p is the number of predictors in the regression model. \n",
    "\n",
    "First, let's create 8 predictors $X$ - 3 we'll use to then create the predictand $y$ and the other will have no relationship to $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we're going to generate the synthetic data with 8 predictors, some with strong relationships to Y, others with none at all \n",
    "\n",
    "# Create the 8 independent variables (X1, X2, ..., X8)\n",
    "X1 = 3 * np.random.rand(n_samples, 1)  # Strong relationship\n",
    "X2 = 2 * np.random.rand(n_samples, 1)  # Strong relationship\n",
    "X3 = np.random.rand(n_samples, 1)      # this will be a weak relationship\n",
    "X4 = np.random.randn(n_samples, 1)     # No relationship (noise)\n",
    "X5 = np.random.randn(n_samples, 1)     # No relationship (noise)\n",
    "X6 = np.random.randn(n_samples, 1)     # No relationship (noise)\n",
    "X7 = np.random.randn(n_samples, 1)     # No relationship (noise)\n",
    "X8 = np.random.randn(n_samples, 1)     # No relationship (noise)\n",
    "\n",
    "# Create the dependent variable Y using only 3 predictors \n",
    "noise = np.random.randn(n_samples, 1) * 3\n",
    "y = 4 + (3 * X1) + (2 * X2) + (0.5 * X3) + noise  # Y depends on just on X1, X2, and X3 plus the magnitide of the noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function for the adjusted $R^2$ now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the adjusted r2 term, which penalizes the model for the number of predictors\n",
    "def adjusted_r2(r2, n, p): # takes the r2 score, the number of observations, and the number of predictors\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do our linear regression again.  We'll cycle through 8 times, each time allowing one more predictor to end the linear regression equation:\n",
    "\n",
    "$\n",
    "Y = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1} + \\hat{\\beta_{2}}X_{2} ... + \\hat{\\beta_{k}}X_{k}\n",
    "$\n",
    "\n",
    "up to $k=8$.  But remember that predictors 4 through 8 are just noise.  We'll calculate the $R^2$ and adjusted $R^2$ to see how the penalty for extra predictors affects our assessment of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now iteratively (and sequentially in for now) add predictors and fit the OLS regression models, while keeping track of how r2 and the adjusted r2 change with each additional predictor\n",
    "X_full = np.hstack([X1, X2, X3, X4, X5, X6, X7, X8]) # horizontally stack all 8 predictors\n",
    "r2_values = [] # make an empty list to receive and append r2 values\n",
    "adjusted_r2_values = []  # make empty list to receive and append adjusted r2 values\n",
    "\n",
    "n_predictors = np.arange(1, 9) # we'll use this below for plotting the step-by-step metrics\n",
    "\n",
    "# now, loop through and use one more predictor each time \n",
    "for i in range(1, 9):\n",
    "    X_subset = X_full[:, :i]  # on each loop use the first i predictors from the matrix X_full\n",
    "    model = LinearRegression() # create the model\n",
    "    model.fit(X_subset, y) # then fit the model\n",
    "    y_pred = model.predict(X_subset) # then predict y from the subset of X we used as predictors\n",
    "    \n",
    "    r2 = r2_score(y, y_pred) # get the r2 score\n",
    "    r2_values.append(r2) # keep it by appending \n",
    "    \n",
    "    adj_r2 = adjusted_r2(r2, n_samples, i)  # get the adjusted r2 score, with i as the number of predictors\n",
    "    adjusted_r2_values.append(adj_r2) # keep it by appending \n",
    "\n",
    "# plot the r2 values and the adjusted r2 \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(n_predictors, r2_values, marker='o', linestyle='-', color='blue',label='R2')\n",
    "plt.plot(n_predictors, adjusted_r2_values, marker='o', linestyle='-', color='red', label='Adjusted R2')\n",
    "plt.title(\"R² vs Number of Predictors\")\n",
    "plt.xlabel(\"Number of Predictors\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"R²\")\n",
    "plt.xticks(n_predictors)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the $R^2$ always slightly increases, even once we are using unrelated noise for predictors.  The adjusted $R^2$ declines after 2 predictors (the two strongest predictors) enter the model.  \n",
    "\n",
    "But, let's think for a moment - how many predictors did we use to create $y$?  How do you explain the different between how we created $y$ and what the adjusted $R^2$ tells you about which model to select? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted $R^2$ attempts to deal with the fact that $R^2$ will always increase, even with the addition of noisy predictors.  But you can see it is rather _ad hoc_.  In order to evaluate whether we've calibrated and selected a 'best' or even 'good' model, we'll need to add another step.  This is **validation** -- we would like to discover if the model we calibrated is still _valid_ when applied to new observations of the predictor.  In statistics, this typically means some form of _cross-validation_, where we withhold some of the available observations, train or calibrate the model on the remained, and then test the ability of the model to predict or estimate the withheld data.  There are various ways to do this and the language used to describe this differs somewhat across fields and between traditional statistics and machine learning, but the goal is the same -- is my model a reasonable and valid one when confronted with data that were not part of the calibration or training of that model?  If the answer is 'no', then it is likely your model is overfit.  Of course, this is seldom a binary 'good' and 'bad' and you will need to be thoughtful in how you determine if a model is valid, useful, and/or meaningful for the purpose you have in mind.\n",
    "\n",
    "Scikit-learn once again helps us here - it has modules for `model_selection` (e.g. how do you choose a valid and 'best' model) called `cross_val_score` which simplify the process of performing cross validation. \n",
    "\n",
    "Documentation of skikit-learn's model_selection is here: https://scikit-learn.org/dev/api/sklearn.model_selection.html\n",
    "\n",
    "And the documentation specifically for cross_val_score is [here](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score). \n",
    "\n",
    "Below, we'll import `cross_val_score` and on each loop through the model we'll call it in the following way:\n",
    "\n",
    "`cv_r2 = np.mean(cross_val_score(model, X_subset, y, cv=5, scoring='r2'))`\n",
    "\n",
    "This line says the following:  for each model configuation, calculate the cross-validation R2 for the `model` using the subset of $X$ predictors `X_subset` to estimate $y$, but iteratively split the data into 5 pieces and only use 4 of them to calibrate the model, then evaluate the fit of the model by predicting $y$ using the withheld 5th of the data$X$ and calculating the $R^2$ value.  This will be done 5 times, withholding a different 5th of the data each time, and we'll use the mean of the five $R^2$ values calculated as the cross-validation score.  \n",
    "\n",
    "What is happening is that we are seeing how well the model does at predicting data that was not part of the modeling training. This gives us some sense of the ability of the model we've built and selected to estimate values it didn't use for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# empty lists to store model metrics\n",
    "r2_values = []\n",
    "adjusted_r2_values = []\n",
    "cv_r2_values = []\n",
    "\n",
    "n_predictors = np.arange(1, 9) # use below for plotting\n",
    " \n",
    "for i in range(1, 9): # loop over, using an increasing subset of predictors again\n",
    "    X_subset = X_full[:, :i]  # Use only the first i predictors\n",
    "    model = LinearRegression() # create model\n",
    "    model.fit(X_subset, y) # fit model\n",
    "    y_pred = model.predict(X_subset) # predict y values from model\n",
    "    \n",
    "    r2 = r2_score(y, y_pred)\n",
    "    r2_values.append(r2)\n",
    "    \n",
    "    adj_r2 = adjusted_r2(r2, n_samples, i)  # i is the number of predictors again\n",
    "    adjusted_r2_values.append(adj_r2)\n",
    "    \n",
    "    # Perform the 5-fold cross-validation and calculate the mean R2 for each model step\n",
    "    cv_r2 = np.mean(cross_val_score(model, X_subset, y, cv=5, scoring='r2'))\n",
    "    cv_r2_values.append(cv_r2)\n",
    "\n",
    "\n",
    "# Plot the R2, Adjusted R2, and Cross-validated R2 values as a function of the number of predictors\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(n_predictors, r2_values, marker='o', linestyle='-', color='blue', label='R2')\n",
    "plt.plot(n_predictors, adjusted_r2_values, marker='o', linestyle='-', color='red', label='Adjusted R2')\n",
    "plt.plot(n_predictors, cv_r2_values, marker='o', linestyle='-', color='green', label='Cross-validated R2')\n",
    "plt.xlabel(\"Number of Predictors\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(n_predictors)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower center')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that after the first 2 predictors enter the model, the cross-validation indicates the model suffers from the addition of additional predictor.  The drop in cross-validation score becomes especially steep after 4 predictors are included, but generally the cross-validation scores are once again indicating that adding more predictors beyond the first 2 degrade the performance of the model to estimate values that were not part of the training set, at least in terms of $R^2$.  \n",
    "\n",
    "Once again, notice that the cross-validation would suggest a model order of 2 predictors is 'best' (again, at least in terms of 5-fold cross validated $R^2$) even though we know that the third predictor also had a role in generating $y$ in this example.  Why do you think that is? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "1. What happens to your model selection if you change the magnitude of the predictors and/or the noise? \n",
    "2. What happens to your model selection if you have an important predictor for $y$ in position 6 or 7 or 8 in the predictor matrix?  How might you deal with this (e.g. when the order of the predictors is not related to their contribution to the predictand)\n",
    "3. What other skill or accuracy metrics might you want to use beyond those above?  Spend some time looking at literature from your field?  How does your field do (cross-)validation of regresssion models? \n",
    "4. What other tools and methods for cross-validation and model selection are available to you (hint: take a look at https://scikit-learn.org/stable/api/sklearn.model_selection.html)\n",
    "5. Using skikit-learn, how might you fit a non-linear regression model? (hint: look at `sklearn.pipeline` and `sklearn.preprocessing`) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
